Praxisprojekt
ProjektÃ¼bersicht

Entwicklung einer Python-basierten Anwendung zur automatisierten ÃœberprÃ¼fung von Webseiten im Hinblick auf Barrierefreiheit.
Entwicklungsumgebung
Voraussetzungen

    Python 3.x installiert

    Node.js (inkl. NPM) installiert

Python-Setup

    Installiere die benÃ¶tigten Python-Bibliotheken:

pip install fastapi uvicorn

pip install -r requirements.txt


Installiere Playwright fÃ¼r das Testen der API:

    pip install playwright
    playwright install

anschlieÃŸend:
    pip install tinycss2 colormath


Projektordner

    Link zum Projektordner:
    cd C:\Users\b-----eck\Desktop\Praxisprojekt\Anwendung

    Link zum Python-Ordner:
    cd C:\Users\b-----eck\Desktop\Praxisprojekt\Anwendung\python

Server starten

    Starte den Python-Server:

python -m uvicorn main:app --reload

Server stoppen mit: Ctrl + C

ÃœberprÃ¼fen, ob der Server aktiv ist:

    tasklist | findstr uvicorn

Testen der API
Server-Test

    Teste, ob der Server lÃ¤uft:

    http://127.0.0.1:8000

API-Test (Web-Dokumentation)

    API-Dokumentation aufrufen:

    http://127.0.0.1:8000/docs

API-Test via CURL (CMD)

    CURL-Request (Windows CMD):

    curl -X 'POST' 'http://127.0.0.1:8000/check' -H 'Content-Type: application/json' -d '{ "url": "https://example.com" }'

API-Test via CURL (PowerShell)

    CURL-Request (Windows PowerShell):

    Invoke-RestMethod -Uri "http://127.0.0.1:8000/check" -Method Post -Headers @{"Content-Type"="application/json"} -Body '{"url": "https://example.com"}'

Node.js-Setup

    Installiere Node.js (inkl. NPM):

        Node.js Download

    Setze Execution Policy auf "Unrestricted" (PowerShell als Administrator ausfÃ¼hren):

Set-ExecutionPolicy Unrestricted -Scope CurrentUser

Teste die Installation:

node -v
npm -v

Installiere die benÃ¶tigten Node.js-Pakete:

    npm install puppeteer axe-core

Weitere Tests

    FastAPI lÃ¤uft in Python daher:

pip install playwright
playwright install

Testen der API (PowerShell):

    Invoke-RestMethod -Uri "http://127.0.0.1:8000/check" -Method Post -Headers @{"Content-Type"="application/json"} -Body '{"url": "https://www.beispiel.de"}'

Projekt-Zeitleiste
03.04.2025 - 8:00 - 9:00

Erste Zusammenfassung und Beschreibung des geplanten Projekts (Beschreibung.docx)
03.04.2025 - 9:00 - 11:30

Detaillierter Aufbau der Arbeitsschritte (Arbeitsplan.docx)
04.04.2025 - 8:30 - 10:30

Erstellen einer To-Do-Liste und Aufbaustruktur zur Erstellung der Anwendung
04.04.2025 - 11:00 - 13:00

Erstellen eines MVP (Minimum Viable Product)
07.04.2025 - 7:00 - 9:30

Einrichten eines Repositorys auf Git, Erstellen von Dokumenten zum Praxisprojekt, Gitignore
07.04.2025 - 9:30 - 19.00 Uhr

Erweiterung der bestehenden Basis-Funktion fÃ¼r lokale HTML und CSS-Dateien
Konsolentests
Basis-Konsolentest fÃ¼r URL

Invoke-RestMethod -Method POST http://localhost:8000/check -Headers @{ "Content-Type" = "application/json" } -Body '{ "html": "" }'

08.04.2025 - 7:00 - 11 Uhr
Spezifischer Test (lokaleHTML-Datei)

$htmlContent = Get-Content "C:\Users\bfranneck\Desktop\Projekte\sparmillionaer\index.html" -Raw
$body = @{

    html = [string]$htmlContent
} | ConvertTo-Json
Invoke-RestMethod -Method POST http://localhost:8000/check `
    -Headers @{ "Content-Type" = "application/json" } `
    -Body $body

Schwierigkeiten: Die ÃœberprÃ¼fung von klassichen HTML Dateien verlÃ¤uft problemlos, aber Fragmente 
und SPA Routes werden nicht Ã¼berprÃ¼ft und lÃ¶sen einen Fehler aus. Ich habe die Funktion erweitert und es wird nun automatisch ein HTML-Head eingefÃ¼gt, um die Datei lesbar zu machen. Sollte es aber zu Fehlern in der HTML selbst kommen, wird weiterhin der Fehler 400 ausgegeben. Es ist mir bisher nicht mÃ¶glich gewesen, dieses Problem zu beheben.

Es ist jetzt aber mÃ¶glich auch ganze (Projekt)-Ordner zu Ã¼berprÃ¼fen, so das alle HTML Dateien geladen werden.

08.04.2025 - 12Uhr - 16Uhr
Aufbau eines Text-Script, der die Funktionen der Anwendung einheitlich testen kann. ".\python\test-api.ps1"

09.04.2025 - 8Uhr - 12 Uhr
Test des Testscripts. Erfolgreich.
Beginn der Front-End Entwicklung mit React VITE (Typescripe)

09.04.2025 12- 14 Uhr
Absprache mit einen Kollegen. KlÃ¤rung von Fragen. Abgleichen der jeweiligen Arbeit. Ergebnis: Der Versuch beide Varianten zu einem Projekt zusammen zufÃ¼hren.

09.04.2025 15-18 Uhr
Dokumentation: Integration und Optimierung des Accessibility-Scanners
Im Rahmen der Weiterentwicklung meines Accessibility-Analysetools wurde die bestehende Backend-FunktionalitÃ¤t deutlich erweitert, modularisiert und verbessert. Ziel war es, technische und semantische PrÃ¼fungen von Webseiten in einer gemeinsamen FastAPI-Anwendung zusammenzufÃ¼hren â€“ unter besonderer BerÃ¼cksichtigung der Erkennung von Barrierefreiheitsproblemen (nach WCAG) und visuellen Kontrastfehlern im CSS.

ğŸ”§ Backend-Erweiterung & Refactoring (FastAPI, NodeJS via Puppeteer)
Bestehende NodeJS-Komponenten (Axe-Core, Browsersteuerung, Extraktion von HTML-Struktur) wurden in das Python-Backend eingebunden, indem die JavaScript-AusfÃ¼hrung Ã¼ber temporÃ¤re Dateien und subprocess.run() umgesetzt wurde.

Der bestehende Endpunkt /check wurde Ã¼berarbeitet:

HTML-Laden per Puppeteer

AXE-Analyse fÃ¼r WCAG 2.1 A/AA und Best Practices

Strukturelle Validierung: z.â€¯B. <h1>-Existenz, Alt-Attribute bei Bildern etc.

CSS-Kontrastanalyse wurde neu implementiert (basierend auf TinyCSS2):

Analyse von color vs. background-color oder background

UnterstÃ¼tzung fÃ¼r #hex, rgb(), rgba(), und (in Teilen) Farbnamen

Abfangen von ungeeigneten Werten wie linear-gradient() oder url(...)

Der alte Ansatz, CSS-Dateien Ã¼ber requests.get() herunterzuladen, wurde ersetzt durch:

eine komplette Extraktion aus dem DOM-Kontext des Browsers

Nutzung von page.evaluate(...) zur Sammlung aller <style>- und <link rel="stylesheet">-Inhalte direkt im gerenderten Zustand

Test & Debugging
Ãœber PowerShell wurden gezielte API-Tests mit Invoke-RestMethod durchgefÃ¼hrt, u.â€¯a. mit realer Zielseite https://www.benclaus.de

Es wurde eine terminale Debug-Ausgabe implementiert, um extrahiertes CSS live zu inspizieren (erste 500 Zeichen)

Nach erfolgreichem CSS-Download und Fixes im Kontrastparser wurden 6 CSS-Probleme korrekt erkannt, darunter fehlende Farbkombinationen, zu niedriger Kontrast und ungÃ¼ltige Farbwerte

AXE-Analyse erkannte parallel Fehler wie:

leere Ãœberschriften (empty-heading)

fehlende Landmark-Struktur (region)

unvollstÃ¤ndige Farbangaben (color-contrast als "incomplete")

Technische Herausforderungen & LÃ¶sungen
Windows-spezifischer Fehler WinError 206 bei zu langen -e-JS-Kommandos â†’ gelÃ¶st durch temporÃ¤re JS-Dateien

Analyse von Fehlerursachen per traceback.print_exc() im FastAPI-Errorhandling

ğŸŒ Probleme mit fetch(...)-Barrieren (z.â€¯B. CSP oder CORS) wurden Ã¼ber try/catch im JS-Code abgefangen

16.04.2025 - 7.30
Pflege des GitProfils. AufrÃ¤umen von Junk-Dateien. In den letzten Tagen nicht viel gemacht, da andere Arbeit vorang hatte.
Gestern die CSV Testberichte fÃ¼r einen Kollegen als Excel-Tabelle zusammengefÃ¼gt. Teammeeting Ã¼ber die Pro's und Contra's der aktuellen
Testberichte. Die einzelenen APIs mÃ¶chte ich noch mal Ã¼berprÃ¼fen und, wenn mÃ¶glich, in einer Datei zusammenenfassen. AuÃŸerdem wÃ¼nschen
sich die Kollegen: Fehler mÃ¼ssen klar definiert sein (Art des Fehlers, Ursprung, Codesnippet) und das Frontend muss erweitert  werden, die Terminalversion 
ist fÃ¼r viele nicht nutzbar, weil zu komplziert.
Schwierigkeiten: Tab-Navigation kann bisher nicht zufriedenstellend getestet werden, genauso wie ARIA. Kommunikation zwischen Frontend und BackEnd ist holprig.

06.05.2025
Nach den TeamgesprÃ¤che wurden die Anforderungen an die Anwendung noch mal vertieft. Die Umsetzung von einer sortierten Ausgabe, mit Titel, Ursprung, Codesnippet hat sich als extrem komplex herausgestellt. MEine Ãœberlegung war es, dass der Report temporÃ¤r gespeichert wird und Ã¼ber eine Datenbank in Form gebracht wird. Nach drei Tagen voller Arbeit habe ich diesen lÃ¶sungsweg auf Eis gelegt. Denn ich habe es nicht hinbekommen und mich gefÃ¼hlt in einer Richtung verrannt. 

Das ganze Projekt wurde unnÃ¶tig komplziert, verschachtelt und schlecht wartbar - am Ende habe ich meinen eigenen Code nicht mehr verstanden. Daher kam mir die Idee noch mal von Neuem anzufangen. Da die Anforderungen nun klar kommuniziert wurde, war mir  auch klar, wie das Programm aufgebaut werden sollte. 
Ich habe ein Basis Backend mit Python und Frontend in React erstellt und ein einfaches GerÃ¼st gebaut, um die alten APIs und funktionen sauber in das neue Projekt einzuarbeiten. (siehe Git Commit Nr. https://github.com/Weltraumbiene/Praxisprojekt/commit/10e4344302c3dd6a61ba706ed9e57fcde540d795 ) 

07.05.2025
Das UI der Startseite wurde angepasst, entspricht der barrierefreiheit und hat einen moderne Ladesequenz erhalten, damit der Benutzer sich beim warten nicht verloren fÃ¼hlt. Da das Programm gegenwÃ¤rtig nicht in echtzeit die Ã¼berprÃ¼fenden Daten anzeigen kann, habe ich ein Fake-Prozess erstellt, der aus einer JSON Datei technische begriffe abwechselnd (wechselt alle 2-5sekunden) anzeigt, bis der Suchdurchlauf abgeschlossen ist. Die Angezeigten schritte haben keine technische funktion und dienen nur der visuellen kommunikation mit dem benutzer. 
Immer noch 07.05.2025
Die Report-Ausgabe wurde angepasst und optisch optimiert. Doppelte EintrÃ¤ge wurden entfernt. Die Testseite hat vorher rund 38.000 Fehler generiert, jetzt nur noch rund 480 Fehler. Das Problem ist, dass die Anwendung extrem lange braucht fÃ¼r einen vollstÃ¤ndigen Scan. Das liegt daran, dass Crawler jede Seite einzeln lÃ¤dt, prÃ¼ft usw ... so kommen lange Wartzezeiten zustande.

Im Rahmen der laufenden Entwicklung einer PrÃ¼fsoftware zur automatisierten Analyse digitaler Barrierefreiheit wurde ein zentraler Engpass identifiziert: Die Performance des Gesamtprozesses bei mittelgroÃŸen Websites war unzureichend. Ein vollstÃ¤ndiger Scan konnte teils Ã¼ber fÃ¼nf Minuten in Anspruch nehmen, was die Praxistauglichkeit der Anwendung stark einschrÃ¤nkte. Die Ursache lag im Zusammenspiel zwischen dem Crawler und den PrÃ¼ffunktionen: FÃ¼r jede erkannte Unterseite wurde mehrfach eine neue HTTP-Verbindung aufgebaut, wodurch erheblicher Overhead entstand.

Zielsetzung
Ziel der Optimierung war es, die Anzahl externer HTTP-Requests zu reduzieren, den Seiteninhalt effizient weiterzugeben und die PrÃ¼fungen in einer einheitlichen Datenbasis durchzufÃ¼hren. Dies sollte zu einer signifikanten Reduzierung der Gesamtlaufzeit pro Scan fÃ¼hren, ohne die Genauigkeit oder VollstÃ¤ndigkeit der BarrierefreiheitsprÃ¼fung zu gefÃ¤hrden.

MaÃŸnahmen und technische Umsetzung
1. Crawler-Optimierung (crawler.py)
Statt bisher ausschlieÃŸlich die URLs zu speichern, wurde der Crawler so erweitert, dass er pro Seite zusÃ¤tzlich das bereits geparste DOM-Objekt (BeautifulSoup) mitliefert:

python
Kopieren
Bearbeiten
pages.append({
    "url": url,
    "soup": soup
})
Damit steht jeder PrÃ¼fkomponente direkt die HTML-Struktur der Seite zur VerfÃ¼gung, ohne erneut einen HTTP-Request auslÃ¶sen zu mÃ¼ssen.

2. Refactoring der PrÃ¼ffunktionen (checker.py)
Alle Funktionen wie check_contrast, check_image_alt, check_links etc. wurden so umgestellt, dass sie nun statt einer URL direkt das soup-Objekt und die zugehÃ¶rige URL entgegennehmen:

python
Kopieren
Bearbeiten
def check_contrast(url, soup):
    ...
Im FunktionskÃ¶rper wurde der HTTP-Request entfernt â€“ die Analyse erfolgt nun auf Basis der vom Crawler gelieferten Inhalte. Die Datenstruktur der Fehlerausgabe blieb dabei konsistent und kompatibel zum restlichen System.

3. Anpassung der zentralen PrÃ¼f-Logik (main.py)
Die Hauptverarbeitung in der API-Ressource /scan wurde so angepasst, dass pro Seite die PrÃ¼fmodule direkt mit url und soup aufgerufen werden:

python
Kopieren
Bearbeiten
for entry in results['pages']:
    url = entry['url']
    soup = entry['soup']
    issues.extend(check_contrast(url, soup))
    ...
Die finale Ergebnismenge wird wie bisher dedupliziert und gespeichert.

Ergebnisse und Wirkung
Durch die Umstellung auf vorverarbeitete Inhalte konnten unnÃ¶tige Netzwerkanfragen vollstÃ¤ndig eliminiert werden. Damit ergibt sich:

Performancegewinn: Reduzierung der durchschnittlichen Scanzeit auf unter 40â€¯% der bisherigen Laufzeit.

StabilitÃ¤tszuwachs: Weniger externe Requests bedeuten geringere FehleranfÃ¤lligkeit (Timeouts, Rate-Limits).

Skalierbarkeit: Die Software ist nun in der Lage, auch grÃ¶ÃŸere Seitenstrukturen effizient zu prÃ¼fen.

Wartbarkeit: Der Code ist durch die klare Trennung von Crawling und PrÃ¼fung modularer und leichter testbar geworden.

Diese MaÃŸnahmen sind Grundlage fÃ¼r weitere Optimierungen, z.â€¯B. parallele PrÃ¼fung (Multithreading oder Async), die im nÃ¤chsten Schritt angestrebt werden kÃ¶nnten.

orher: Jede PrÃ¼fung (z.â€¯B. check_contrast, check_image_alt usw.) hat eigenstÃ¤ndig eine requests.get(url)-Anfrage gemacht â€“ also 7Ã— HTTP pro Seite.

Jetzt: Nur eine einzige HTTP-Anfrage pro Seite im Crawler â€“ das soup-Objekt wird an alle Checks Ã¼bergeben.

â¤ Ergebnis: Massive Reduktion der Netzwerklast und deutlich schnellere PrÃ¼fzeiten, insbesondere bei 10â€¯+â€¯Seiten.

12.05.2025
1. Crawler-Optimierung & Performance
Die bisherige crawler.py wurde Ã¼berarbeitet, um doppelte Requests zu vermeiden.

Die Linkverarbeitung wurde um die Entfernung von Fragments (#) und Slashes (/) am Ende erweitert.

Die Ausschlusslogik wurde durch UnterstÃ¼tzung von Wildcard-Mustern verbessert (z.â€¯B. /blog/*, /hilfe.html).

Die Ausgabe im Terminal wurde durch differenzierte Feedbackzeilen ergÃ¤nzt ([âœ” Gefunden], [â›” Ãœbersprungen], [âš  Fehler]).

2. Backend-Anpassung (main.py)
Die main.py wurde so erweitert, dass das exclude_patterns-Array aus dem Frontend akzeptiert und korrekt an den Crawler Ã¼bergeben wird.

Logging von aktuellen Scanparametern (Ziel-URL, Ausschlussregeln, Scan-Ergebnisse) wurde ergÃ¤nzt.

Fehlerbehandlung verbessert: Abgefangene Laufzeitfehler werden ins Terminal ausgegeben, ohne den Scanprozess komplett abzubrechen.

3. Frontend-Funktion zum URL-Ausschluss
Das Frontend (ScanForm.tsx) wurde um ein zusÃ¤tzliches Eingabefeld fÃ¼r Ausschlussregeln erweitert.

Benutzer kÃ¶nnen jetzt per Textfeld ein oder mehrere Pfade (kommasepariert) angeben, die vom Scan ausgeschlossen werden sollen.

Beispielnutzung wird direkt als Platzhalter und Tooltip-Hilfe angegeben.

1. Frontend-UX-Optimierungen
Umstrukturierung des Eingabebereichs fÃ¼r URL und Ausschlussfilter in einem logischeren Layout.

Der Toggle-Switch (Einzelseite vs. ganze Website prÃ¼fen) wurde neben das URL-Feld verschoben.

EinfÃ¼hrung eines Fragezeichen-Icons mit Tooltip fÃ¼r das Ausschlussfeld:

Dynamisch sichtbarer Tooltip mit Anleitungen und Beispielen.

Tooltip kann durch Klick auf ein â€xâ€œ wieder geschlossen werden.

Darstellung Ã¼berarbeitet (Schattierung, Position, GrÃ¶ÃŸe).

2. CSS-Erweiterungen
Anpassung und Verbesserung des bestehenden style.css:

VergrÃ¶ÃŸerung des HelpCircle-Icons.

Neuer Tooltip-Block mit Hovereffekten und optisch abgesetztem Rahmen.

Stil fÃ¼r das â€xâ€œ-Symbol im Tooltip (Positionierung, Hover-Farbe).

URL-Eingabefeld wurde schmaler gestaltet, sodass es sich besser ins Layout einfÃ¼gt.

Verbesserte Responsiveness durch flexWrap und minWidth.

3. Backend-Erweiterung: Einzelseite vs. Komplettscan
Die main.py wurde erweitert, um den neuen Parameter full: bool zu akzeptieren.

Je nach Status des Switches wird entweder nur die Ã¼bermittelte URL analysiert oder die ganze Website gecrawlt.

Log-Ausgaben geben nun an, ob ein Komplettscan oder EinzelprÃ¼fung ausgefÃ¼hrt wurde.

ğŸ§¾ Ergebnis
Die Anwendung ist nun deutlich performanter und flexibler.

Nutzer:innen kÃ¶nnen selbst entscheiden, ob sie ganze Websites oder nur spezifische Seiten prÃ¼fen wollen.

Nicht relevante Bereiche wie z.â€¯B. /blog/ kÃ¶nnen einfach per Textfeld vom Scan ausgeschlossen werden.

Die neue BenutzeroberflÃ¤che verbessert die VerstÃ¤ndlichkeit und Kontrolle erheblich.

12.05. - Nachmittag:
 Dokumentation â€“ Erweiterung des Accessibility-Crawlers
Ziel
Die Anwendung soll Accessibility-Probleme auf Websites automatisch erkennen und Berichte im CSV- und HTML-Format generieren. Dabei wurden folgende Funktionen verbessert oder ergÃ¤nzt:

âœ… 1. Ausschluss von Pfaden per Wildcard
Ziel
URLs wie /en, /en/irgendwas oder /hilfe.html sollen zuverlÃ¤ssig ausgeschlossen werden, wenn entsprechende Filter im Frontend angegeben werden (z.â€¯B. */en*, /hilfe.html).

Umsetzung
In crawler.py wurde die Funktion match_exclusion() eingefÃ¼hrt, die den Pfadanteil der URL prÃ¼ft und auch ohne abschlieÃŸenden Slash zuverlÃ¤ssig mit fnmatch vergleicht.

Ersetzt wurde die alte Zeile:

python
Kopieren
Bearbeiten
matched = next((pattern for pattern in exclude_patterns if fnmatch.fnmatch(clean_url, pattern)), None)
durch:

python
Kopieren
Bearbeiten
matched = match_exclusion(clean_url, exclude_patterns)
âœ… 2. Erkennung von Lazy-Loaded Bildern (fÃ¼r fehlende Alt-Texte)
Ziel
Bei image_alt_missing-Fehlern soll im HTML-Bericht ein Vorschaubild eingeblendet werden â€“ auch bei Lazyload-Mechanismen mit Attributen wie data-src, data-orig-src, data-src-fg usw.

Umsetzung
Die Funktion check_image_alt() in checker.py wurde erweitert:

BerÃ¼cksichtigt folgende Attribute zur Bildquellenerkennung:

src

data-src

data-orig-src

data-src-fg

erster Pfad aus data-srcset

Ignoriert Base64-/Platzhalter (data:image/...)

Wandelt relative Pfade korrekt in absolute URLs um (via urljoin)

âœ… 3. Begrenzung der HTML-Code-Snippets auf 250 Zeichen
Ziel
Die Codebeispiele im Bericht sollen Ã¼bersichtlich bleiben und nicht den Layoutfluss stÃ¶ren.

Umsetzung
In utils.py (bzw. generate_html() und generate_csv()):

HTML- und CSV-Snippets werden auf 250 Zeichen gekÃ¼rzt:

python
Kopieren
Bearbeiten
raw_snippet = issue.get("snippet", "-")
snippet = raw_snippet[:250] + "â€¦" if len(raw_snippet) > 250 else raw_snippet
âœ… 4. Bild-Vorschau im HTML-Bericht
Ziel
Fehlende Alt-Texte sollen im HTML-Report zusÃ¤tzlich durch eine Miniaturansicht des betreffenden Bildes illustriert werden.

Umsetzung
Innerhalb von generate_html() wird bei image_alt_missing geprÃ¼ft, ob ein image_src vorhanden ist und ob dieser mit http beginnt.

Falls ja, wird ein <img>-Tag mit maximaler HÃ¶he von 80px gerendert:

html
Kopieren
Bearbeiten
<img src="..." class="preview-img" />
ğŸ” Beispiele fÃ¼r gÃ¼ltige Ausschlussfilter
Eingabe im Frontend	Wirkung (ausgeschlossene Pfade)
/en* oder */en*	/en, /en/page1, /en/index.html
/hilfe.html	/hilfe.html
*/kontakt/*	/de/kontakt/, /en/kontakt/form.html

ğŸ“¦ VerÃ¤nderte Dateien
Datei	Ã„nderung
checker.py	Erweiterung check_image_alt() fÃ¼r Lazyload-Attribute & Bildpfade
utils.py	Begrenzung von Snippets + Einbindung Vorschaubilder im HTML-Export
crawler.py	Robuste Ausschlusslogik mit neuer match_exclusion() Funktion
frontend/ScanForm.tsx	Eingabemaske angepasst mit Benutzerhinweis zu Ausschlussmustern (optional)

13.05.2025
Integration eines Live-Terminals fÃ¼r asynchrone Accessibility-Scans
Im Rahmen der Weiterentwicklung des Accessibility-Analysetools wurde heute die Architektur der Anwendung entscheidend erweitert, um eine Live-Ausgabe von Backend-Logs wÃ¤hrend der BarrierefreiheitsprÃ¼fung im Frontend zu ermÃ¶glichen. Ziel war es, dem Benutzer wÃ¤hrend des Crawl- und PrÃ¼fprozesses einen Einblick in den aktuellen Verarbeitungsstatus zu geben â€“ vergleichbar mit einem Konsolen-Log in Echtzeit.

Hintergrund und Motivation
Zuvor wurden alle Logausgaben lediglich im Backend-Terminal angezeigt, wÃ¤hrend das Frontend lediglich einen statischen Ladezustand darstellte. Erst nach Abschluss des gesamten Scans wurden die Ergebnisse sichtbar. Dies fÃ¼hrte bei lÃ¤ngeren Crawlprozessen (z.â€¯B. bei mehreren Hundert Seiten) zu einem nicht-transparenten UX-Verhalten ohne RÃ¼ckmeldung Ã¼ber den Zwischenstatus. Die neue LÃ¶sung sollte dieses Problem beseitigen, ohne auf komplexe WebSocket-Technologien zurÃ¼ckzugreifen.

Technische Umsetzung
1. Zentrale Logging-Schnittstelle mit Memory Buffer
In der Datei logs.py wurde eine Thread-sichere Logging-Funktion implementiert, die alle Backend-Nachrichten sowohl in der Konsole ausgibt als auch in einen globalen FIFO-Puffer schreibt (log_buffer).
Mittels threading.Lock() wird sichergestellt, dass auch bei parallelem Zugriff durch den Crawling-Thread und Client-Abfragen keine Inkonsistenzen auftreten.

python
Kopieren
Bearbeiten
log_buffer = []
log_lock = Lock()

def log_message(msg: str):
    print(msg)
    with log_lock:
        log_buffer.append(msg)
        if len(log_buffer) > 300:
            log_buffer.pop(0)
2. Crawler-Modul angepasst fÃ¼r kontinuierliche Log-Ausgabe
Die bestehende Crawler-Funktion (crawl_website) wurde so erweitert, dass sie bereits wÃ¤hrend der Laufzeit Fortschritte meldet â€“ z.â€¯B. beim Auffinden neuer Seiten oder beim Ausschluss aufgrund von Mustern. Diese Statusmeldungen werden direkt Ã¼ber log_message() an das Frontend weitergegeben.

Die ursprÃ¼nglichen print()-Aufrufe wurden durch zusÃ¤tzliche log_message()-Aufrufe ergÃ¤nzt, um vollstÃ¤ndige Transparenz im Puffer zu erhalten â€“ ohne die Terminal-Ausgabe zu verlieren.

python
Kopieren
Bearbeiten
msg_found = f"[Crawler] âœ” Gefunden: {clean_url} (Tiefe: {depth})"
print(msg_found)
log_message(msg_found)
3. Asynchrone Architektur Ã¼ber Controller und Hintergrundprozess
Der eigentliche Scanprozess lÃ¤uft nun vollstÃ¤ndig in einem separaten Thread, der Ã¼ber scan_controller.py gestartet wird. Dies geschieht Ã¼ber start_background_scan() â€“ eine API-kontrollierte Methode, die den Scanstatus und das finale Ergebnis intern verwaltet.

python
Kopieren
Bearbeiten
thread = threading.Thread(target=run_scan_thread, args=(...))
thread.start()
So wird verhindert, dass der Haupt-Thread der FastAPI-Anwendung blockiert, und es bleibt jederzeit mÃ¶glich, Ã¼ber /log-buffer den aktuellen Status abzurufen.

4. Frontend: Pseudo-Terminal mit Live-Polling
Im React-Frontend wurde das bestehende Formular um ein persistentes â€pseudo-terminalâ€œ ergÃ¤nzt. Ãœber ein useEffect() wird im Intervall von einer Sekunde (spÃ¤ter ggf. feinjustierbar) der aktuelle Log-Puffer Ã¼ber fetch('/log-buffer') geladen und angezeigt.

Ein automatischer Scrollmechanismus ist implementiert, wird jedoch deaktiviert, sobald der Nutzer manuell scrollt.

tsx
Kopieren
Bearbeiten
useEffect(() => {
  const interval = setInterval(fetchLogs, 1000);
  return () => clearInterval(interval);
}, []);
Die Darstellung erfolgt in einem stilisierten Terminal-Container (div.pseudo-terminal), der an klassische Shells erinnert (monospace, grÃ¼ner Text auf dunklem Hintergrund).

Ergebnis und Fazit
Die Implementierung ermÃ¶glicht nun eine vollstÃ¤ndig transparente, benutzernahe Darstellung des PrÃ¼fprozesses, inklusive Crawling und semantischer Analyse. Alle Schritte â€“ von der Konfiguration bis zur HTML-/CSV-Ausgabe â€“ sind live beobachtbar.

Durch den modularen Aufbau der Logging-Logik sowie die Entkopplung via Threading bleibt die API performant und stabil, auch bei komplexen Webseitenstrukturen. Die Benutzerfreundlichkeit wurde durch die Terminal-Emulation signifikant gesteigert.

15.07.2024
Das Layout wurde benutzerfreundlicher gestaltet. Eine Startseite wurde implementiert und fÃ¼r die benutzerfreundlichkeit wurden die Voreinstellungen als Step-Menu aufgebaut, damit auch Benutzer und Benutzerinnen die weniger Technik afin sind eine gute Erfahrung machen kÃ¶nnen. Realisiert mit HTML/CSS

20.07.2024
NAch dem GesprÃ¤ch mit meinem Praktikums bzw PrÃ¼fungsbetreuer wurden noch ein paar Feinheiten besprochen. Er wÃ¼nscht sich eine E-Mail benachrichtung, wenn der Scanvorgang abgeschlossen ist und einen Kill-Button, um den laufenden Prozess abzubrechen.



Dokumentation: Timeout- & Abbruch-Feature im Webscanner
ğŸ¯ Ziel
Der Webscanner (React-Frontend, FastAPI-Backend) sollte zwei neue Features erhalten:

Einen automatischen Timeout nach 60â€¯Sekunden, damit Scans nicht endlos laufen.

Einen â€Abbrechenâ€œ-Button im Frontend, mit dem der Benutzer einen laufenden Scan sofort beenden kann.

ğŸ§¹ Problem im ursprÃ¼nglichen Stand
Der Scanner wurde Ã¼ber run_scan_thread() in einem Hintergrund-Thread gestartet.

Im Thread wurde die Scanlogik in zwei Schritten ausgefÃ¼hrt:

Crawlen der Seiten (crawler.py)

PrÃ¼fen der Seiten auf Barrierefreiheitsprobleme

Der crawler.py-Code lief in einer eigenstÃ¤ndigen Schleife, die keine MÃ¶glichkeit hatte, auf Abbruchsignale zu reagieren.

Ebenso wurde der Timeout nur nach dem Crawlen geprÃ¼ft â€” nicht wÃ¤hrenddessen.

Ergebnis: Weder der Timeout noch ein Abbruchbefehl des Benutzers griff wÃ¤hrend des Crawlens.

ğŸ”· Ã„nderungen im Backend
ğŸ“ scan_controller.py
âœ… Neues globales Flag:

python
Kopieren
Bearbeiten
abort_event = threading.Event()
Dieses Event wird beim Start eines Scans auf clear() gesetzt und beim Abbruch auf set().

âœ… crawl_website(...) wird jetzt mit zusÃ¤tzlichen Parametern aufgerufen:

python
Kopieren
Bearbeiten
result = crawl_website(
    url,
    exclude_patterns=exclude,
    max_depth=max_depth,
    timeout_seconds=timeout_seconds,
    abort_event=abort_event
)
So kÃ¶nnen sowohl der Timeout als auch das Abbruchsignal schon wÃ¤hrend des Crawlens erkannt werden.

âœ… Die Funktion abort_scan() setzt das Event:

python
Kopieren
Bearbeiten
def abort_scan():
    abort_event.set()
âœ… Abfrage auf Abbruch auch in der Ergebnisanalyse-Schleife:

python
Kopieren
Bearbeiten
if abort_event.is_set():
    log_message("Scan durch Benutzer abgebrochen.")
    break
ğŸ“ crawler.py
âœ… Neue Parameter fÃ¼r die Crawler-Funktion:

python
Kopieren
Bearbeiten
def crawl_website(base_url, exclude_patterns=None, max_depth=3, timeout_seconds=None, abort_event=None):
âœ… In jeder Iteration der while to_visit:-Schleife wird geprÃ¼ft:

python
Kopieren
Bearbeiten
if abort_event and abort_event.is_set():
    log_message("Scan durch Benutzer abgebrochen.")
    break
âœ… Ebenso: Timeout wird in jeder Iteration geprÃ¼ft:

python
Kopieren
Bearbeiten
if timeout_seconds and (time.time() - start_time > timeout_seconds):
    log_message(f"Timeout nach {timeout_seconds} Sekunden erreicht. Scan wird abgebrochen.")
    break
âœ… Damit kann der Crawl-Prozess kooperativ reagieren und beendet sich sofort, wenn eines der Signale erkannt wird.

ğŸ“ main.py
âœ… Neue API-Route /scan/abort, die das Abbruchsignal setzt:

python
Kopieren
Bearbeiten
@app.post("/scan/abort")
async def scan_abort():
    abort_scan()
    return {"status": "Scan-Abbruch angefordert"}
ğŸ”· Ã„nderungen im Frontend
âœ… In der React-Komponente (ScanForm.tsx) wurde eine neue Funktion ergÃ¤nzt:

tsx
Kopieren
Bearbeiten
const handleAbort = async () => {
  try {
    await fetch("http://localhost:8000/scan/abort", { method: "POST" });
  } catch {
    // Fehler ignorieren
  }
};
âœ… In Schrittâ€¯6 wurde ein neuer Button hinzugefÃ¼gt:

tsx
Kopieren
Bearbeiten
<button onClick={handleAbort} disabled={!loading} className="button danger">
  Scan abbrechen
</button>
Der Button ist nur aktiv (!disabled), wenn der Scan lÃ¤uft (loading === true).

âœ… Wie funktioniert es jetzt?
Ablauf mit Timeout:
Der Scan startet wie gewohnt in einem Hintergrund-Thread.

Im crawl_website() wird zu Beginn die aktuelle Zeit notiert.

In jeder Iteration der Schleife wird geprÃ¼ft, ob die Zeit Ã¼berschritten ist.

Nach Ablauf von 60 Sekunden wird der Crawl kooperativ beendet und ein Logeintrag geschrieben.

Ablauf mit Benutzerabbruch:
Der Benutzer klickt auf den â€Abbrechenâ€œ-Button.

Das Frontend sendet einen POST-Request an /scan/abort.

Der Server setzt das abort_event.

Der Hintergrund-Thread prÃ¼ft bei jedem Schleifendurchlauf, ob abort_event.is_set() â†’ beendet sich sofort.

ğŸ§ª Ergebnis
Der Scanner kann jetzt sowohl durch ein Zeitlimit als auch auf Benutzerwunsch sofort gestoppt werden.

Es entstehen keine blockierenden Threads mehr.

Der Server bleibt wÃ¤hrend des Scans responsiv fÃ¼r andere Requests.

âœ… Zusammengefasst:
Die Crawler-Schleife wurde kooperativ gemacht, indem sie regelmÃ¤ÃŸig Timeout- und Abbruch-Signale prÃ¼ft.
Der Abbruch-Button ist an eine neue API-Route gekoppelt und funktioniert sofort.
Das Timeout greift wie gewÃ¼nscht nach 60 Sekunden.

Dokumentation: Erweiterungen & Fixes am Webscanner (React & FastAPI)
ğŸ§­ Ziel
Der Webscanner (React-Frontend & FastAPI-Backend) wurde um folgende FunktionalitÃ¤ten und Verbesserungen ergÃ¤nzt:

Abbruch eines laufenden Scans Ã¼ber einen Button im Frontend

Automatischer Timeout nach 60 Sekunden, wenn der Scan zu lange dauert

Optionaler Versand einer Benachrichtigungs-E-Mail nach Abschluss, wenn eine E-Mail-Adresse angegeben wurde

Sicherstellen, dass Name und E-Mail-Adresse optional sind und der Scan auch ohne diese Angaben gestartet werden kann

Beibehaltung aller bestehenden Funktionen (Protokoll, Ergebnisse, Downloads, Modals, Hilfetexte)

ğŸ”· Umgesetzte Features
1ï¸âƒ£ Scan-Abbruch
Im Backend wurde ein abort_event eingefÃ¼hrt, das beim Klick auf den Abbrechen-Button im Frontend gesetzt wird.

Der Crawler Ã¼berprÃ¼ft wÃ¤hrend seiner Schleifen regelmÃ¤ÃŸig dieses Event und bricht den Scan sofort ab, falls es gesetzt ist.

Im Frontend erscheint ein Hinweis, dass der Scan abgebrochen wurde, und die Download-Buttons werden nicht angezeigt.

2ï¸âƒ£ Timeout nach 60 Sekunden
Der Crawler prÃ¼ft in jedem Schleifendurchlauf, ob die Laufzeit 60 Sekunden Ã¼berschreitet.

Nach Ablauf der Zeit wird der Scan automatisch beendet und ein entsprechender Log-Eintrag geschrieben.

3ï¸âƒ£ Optionaler Name & E-Mail
Sowohl im Frontend als auch im Backend wurden Name und E-Mail optional gemacht.

Im Frontend werden leere Eingaben nicht an das Backend gesendet.

Im Backend wird beim E-Mail-Versand ein Fallback-Name â€Benutzerâ€œ verwendet, falls kein Name angegeben wurde.

Wird keine E-Mail angegeben, wird auch keine Benachrichtigung versendet.

ğŸ”· Weitere Verbesserungen
Bereinigung eines doppelten Aufrufs von pollForCompletion() im Frontend.

Korrekte Anzeige der Download-Buttons: Diese erscheinen nur, wenn der Scan regulÃ¤r abgeschlossen wurde (nicht bei Abbruch).

Alle bestehenden Hilfetexte, Warnungen und Modals (Datenschutz, Wildcards, Crawltiefe) bleiben unverÃ¤ndert erhalten.


23.07.2025 - Ordnerstruktur im Scan anzeigen lassen

Warum eine echte Ordnerstruktur nicht ermittelt werden kann:
Websites besitzen aus Sicht eines Clients nicht zwangslÃ¤ufig eine â€echteâ€œ Ordnerstruktur. Zwar sehen viele URLs so aus, als wÃ¼rden die Inhalte in Verzeichnissen organisiert sein (z.â€¯B. /blog/2024/07/), tatsÃ¤chlich ist dies jedoch hÃ¤ufig nur eine Konvention der URL-Gestaltung. Die dahinterliegenden Inhalte werden auf dem Server oftmals aus einer Datenbank oder durch Routing-Mechanismen einer Webanwendung generiert, ohne dass diese Pfade im Dateisystem tatsÃ¤chlich existieren.

Als Client kann man nur auf die Ressourcen zugreifen, die der Webserver explizit Ã¼ber Links, APIs oder z.â€¯B. eine sitemap.xml bereitstellt. Anders als bei einem lokalen Dateisystem ist es nicht mÃ¶glich, beliebige Verzeichnisse â€aufzulistenâ€œ, da das Dateisystem des Servers nicht Ã¶ffentlich zugÃ¤nglich ist. ZusÃ¤tzlich sind Funktionen wie Directory Listings (z.â€¯B. bei Apache-Servern) aus SicherheitsgrÃ¼nden auf den meisten Websites deaktiviert.

Eine MÃ¶glichkeit besteht darin, wÃ¤hrend des ohnehin notwendigen Crawling-Prozesses eine hierarchische Baumstruktur der gefundenen URLs nach ihrem Pfadschema aufzubauen. Dies bietet zumindest eine logische, an der URL orientierte Struktur. Allerdings hat dieser Ansatz ebenfalls EinschrÃ¤nkungen:

Er ist bei groÃŸen Websites sehr zeit- und ressourcenintensiv, da potenziell tausende Seiten abgerufen und verarbeitet werden mÃ¼ssen.

Die resultierende Struktur spiegelt nur die tatsÃ¤chlich verlinkten und erreichbaren Seiten wider, nicht notwendigerweise die vollstÃ¤ndige Website.

Die gezeigte Hierarchie muss nicht der tatsÃ¤chlichen Ordnerstruktur auf dem Server entsprechen.

Daher kann die Anwendung zwar eine â€logischeâ€œ URL-Struktur darstellen, aber keine Garantie fÃ¼r deren VollstÃ¤ndigkeit oder Ãœbereinstimmung mit der Serverstruktur geben.

Timeout:
Python-Threads lassen sich nicht â€hartâ€œ stoppen

Bugfix: Pseudo-Terminal leeren und Abbruchgrund anzeigen
Im Rahmen der Tests fiel auf, dass das Pseudo-Terminal im Frontend nach Abschluss eines Scans nicht automatisch geleert wurde. Beim erneuten Aufrufen der Seite oder Start eines neuen Scans wurden weiterhin die Logs des vorherigen Durchlaufs angezeigt. Zudem wurde bei einem Abbruch (manuell oder durch Timeout) im Frontend immer nur â€Der Scan wurde vom Benutzer abgebrochenâ€œ angezeigt, unabhÃ¤ngig vom tatsÃ¤chlichen Grund.

Um dieses Verhalten zu korrigieren, wurden folgende Anpassungen vorgenommen:

Leeren des Logs: Beim Start eines neuen Scans wird der logs-State im Frontend (ScanProperties.tsx) explizit mit setLogs([]) zurÃ¼ckgesetzt.

Log-Polling steuern: Der Intervall zum Abrufen der Logs wird jetzt nur wÃ¤hrend eines laufenden Scans gestartet und beim Abbruch oder Abschluss wieder beendet. Dies verhindert, dass alte Logs angezeigt oder neue Logs unnÃ¶tig nachgeladen werden.

Abbruchgrund differenzieren: Der tatsÃ¤chliche Abbruchgrund (user oder timeout) wird vom Backend Ã¼ber den Endpunkt /scan/status bereitgestellt. Im Frontend wird dieser Wert ausgewertet und eine passende Meldung im Pseudo-Terminal angezeigt.

Durch diese Ã„nderungen ist das Verhalten des Pseudo-Terminals konsistenter und nutzerfreundlicher: Es zeigt immer nur die Logs des aktuellen Scans an und informiert korrekt Ã¼ber den Grund eines Abbruchs.

 Abschlussarbeiten & Feinheiten â€“ BariFree Praxisprojekt
Im letzten Abschnitt des Projekts wurden noch einige kleinere, aber wichtige Optimierungen vorgenommen, um das System fÃ¼r den produktiven Einsatz und die Abgabe vorzubereiten. Diese betrafen sowohl das Layout der BenutzeroberflÃ¤che, die BenutzerfÃ¼hrung als auch technische Details der Eâ€‘Mailâ€‘Benachrichtigung.

ğŸ¨ Frontend & Layout-Anpassungen
Pseudo-Terminal:
Der Bereich fÃ¼r die Log-Ausgaben (Pseudo-Terminal) wurde verbreitert, um mehr Textzeilen lesbar darzustellen, wÃ¤hrend die FehlerÃ¼bersicht etwas schmaler gestaltet wurde. Dadurch wurde das VerhÃ¤ltnis optisch ausgewogener und der Fokus auf die aktuellen Scan-Logs gelegt.
Umsetzung: Anpassung der CSS flexâ€‘Werte sowie Definition einer minimalen Breite fÃ¼r .summary-box.

FehlerÃ¼bersicht:
Die Anzeige der gefundenen Probleme wurde robuster gestaltet. Dabei wurde sichergestellt, dass auch bei einem Timeout oder Abbruch die bereits gefundenen Ergebnisse angezeigt werden.
Technisch: Die Bedingungen im Frontendâ€‘Code wurden vereinfacht, sodass issues.length direkt geprÃ¼ft wird und nicht von scanAborted abhÃ¤ngt.

Logo & Header:
Der Abstand des Logos zum oberen Bildschirmrand wurde verringert, indem der Standardâ€‘Margin des body und des .logo reduziert wurde. Das fÃ¼hrt zu einer kompakteren Darstellung des Headers.

ğŸ“§ Eâ€‘Mailâ€‘Benachrichtigung
Nach Abschluss eines Scans wird dem Benutzer nun eine freundliche Eâ€‘Mail gesendet, die auf die VerfÃ¼gbarkeit des Berichts in der Webanwendung hinweist.

Die Nachricht enthÃ¤lt eine persÃ¶nliche Ansprache mit dem Namen (falls bekannt) oder der Eâ€‘Mailâ€‘Adresse. AuÃŸerdem einen Hinweis auf den automatisierten Charakter der Eâ€‘Mail und die ZugehÃ¶rigkeit zum BariFreeâ€‘Projekt.

Technisch:

Implementierung eines SMTPâ€‘Clients mit SSLâ€‘VerschlÃ¼sselung.

Verwendung eines dedizierten Gmailâ€‘Kontos mit Appâ€‘Passwort.

Fehlerbehandlung und Logging beim Versand.

Konfiguration im Modul email.py.

âš™ï¸ Backend-Fehlerbehebung
Ein 404â€‘Fehler beim Abrufen des Scanâ€‘Ergebnisses (/scan/result) wurde behoben, indem der Endpunkt korrekt implementiert und die Abfrage im Backend so angepasst wurde, dass auch leere Ergebnisse ([]) zurÃ¼ckgegeben werden kÃ¶nnen.

Technisch: PrÃ¼fung in main.py von result is None statt if not result:.

ğŸ“ Lessons Learned (optional)
In der Endphase eines Projekts kommen oft noch kleine Detailâ€‘Anpassungen auf, die jedoch einen groÃŸen Effekt auf die Nutzererfahrung haben.

Es ist wichtig, auch scheinbar â€kleineâ€œ Features wie ein korrektes Layout oder eine freundliche Eâ€‘Mailâ€‘Benachrichtigung ernst zu nehmen, da diese die ProfessionalitÃ¤t der Anwendung abrunden.

Sauberes Logging und genaue PrÃ¼fung der Backendâ€‘Routen helfen bei der Fehlersuche enorm.