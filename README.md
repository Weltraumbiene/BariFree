Praxisprojekt
Projekt√ºbersicht

Entwicklung einer Python-basierten Anwendung zur automatisierten √úberpr√ºfung von Webseiten im Hinblick auf Barrierefreiheit.
Entwicklungsumgebung
Voraussetzungen

    Python 3.x installiert

    Node.js (inkl. NPM) installiert

Python-Setup

    Installiere die ben√∂tigten Python-Bibliotheken:

pip install fastapi uvicorn

pip install -r requirements.txt


Installiere Playwright f√ºr das Testen der API:

    pip install playwright
    playwright install

anschlie√üend:
    pip install tinycss2 colormath


Projektordner

    Link zum Projektordner:
    cd C:\Users\b-----eck\Desktop\Praxisprojekt\Anwendung

    Link zum Python-Ordner:
    cd C:\Users\b-----eck\Desktop\Praxisprojekt\Anwendung\python

Server starten

    Starte den Python-Server:

python -m uvicorn main:app --reload

Server stoppen mit: Ctrl + C

√úberpr√ºfen, ob der Server aktiv ist:

    tasklist | findstr uvicorn

Testen der API
Server-Test

    Teste, ob der Server l√§uft:

    http://127.0.0.1:8000

API-Test (Web-Dokumentation)

    API-Dokumentation aufrufen:

    http://127.0.0.1:8000/docs

API-Test via CURL (CMD)

    CURL-Request (Windows CMD):

    curl -X 'POST' 'http://127.0.0.1:8000/check' -H 'Content-Type: application/json' -d '{ "url": "https://example.com" }'

API-Test via CURL (PowerShell)

    CURL-Request (Windows PowerShell):

    Invoke-RestMethod -Uri "http://127.0.0.1:8000/check" -Method Post -Headers @{"Content-Type"="application/json"} -Body '{"url": "https://example.com"}'

Node.js-Setup

    Installiere Node.js (inkl. NPM):

        Node.js Download

    Setze Execution Policy auf "Unrestricted" (PowerShell als Administrator ausf√ºhren):

Set-ExecutionPolicy Unrestricted -Scope CurrentUser

Teste die Installation:

node -v
npm -v

Installiere die ben√∂tigten Node.js-Pakete:

    npm install puppeteer axe-core

Weitere Tests

    FastAPI l√§uft in Python daher:

pip install playwright
playwright install

Testen der API (PowerShell):

    Invoke-RestMethod -Uri "http://127.0.0.1:8000/check" -Method Post -Headers @{"Content-Type"="application/json"} -Body '{"url": "https://www.beispiel.de"}'

Projekt-Zeitleiste
03.04.2025 - 8:00 - 9:00

Erste Zusammenfassung und Beschreibung des geplanten Projekts (Beschreibung.docx)
03.04.2025 - 9:00 - 11:30

Detaillierter Aufbau der Arbeitsschritte (Arbeitsplan.docx)
04.04.2025 - 8:30 - 10:30

Erstellen einer To-Do-Liste und Aufbaustruktur zur Erstellung der Anwendung
04.04.2025 - 11:00 - 13:00

Erstellen eines MVP (Minimum Viable Product)
07.04.2025 - 7:00 - 9:30

Einrichten eines Repositorys auf Git, Erstellen von Dokumenten zum Praxisprojekt, Gitignore
07.04.2025 - 9:30 - 19.00 Uhr

Erweiterung der bestehenden Basis-Funktion f√ºr lokale HTML und CSS-Dateien
Konsolentests
Basis-Konsolentest f√ºr URL

Invoke-RestMethod -Method POST http://localhost:8000/check -Headers @{ "Content-Type" = "application/json" } -Body '{ "html": "" }'

08.04.2025 - 7:00 - 11 Uhr
Spezifischer Test (lokaleHTML-Datei)

$htmlContent = Get-Content "C:\Users\bfranneck\Desktop\Projekte\sparmillionaer\index.html" -Raw
$body = @{

    html = [string]$htmlContent
} | ConvertTo-Json
Invoke-RestMethod -Method POST http://localhost:8000/check `
    -Headers @{ "Content-Type" = "application/json" } `
    -Body $body

Schwierigkeiten: Die √úberpr√ºfung von klassichen HTML Dateien verl√§uft problemlos, aber Fragmente 
und SPA Routes werden nicht √ºberpr√ºft und l√∂sen einen Fehler aus. Ich habe die Funktion erweitert und es wird nun automatisch ein HTML-Head eingef√ºgt, um die Datei lesbar zu machen. Sollte es aber zu Fehlern in der HTML selbst kommen, wird weiterhin der Fehler 400 ausgegeben. Es ist mir bisher nicht m√∂glich gewesen, dieses Problem zu beheben.

Es ist jetzt aber m√∂glich auch ganze (Projekt)-Ordner zu √ºberpr√ºfen, so das alle HTML Dateien geladen werden.

08.04.2025 - 12Uhr - 16Uhr
Aufbau eines Text-Script, der die Funktionen der Anwendung einheitlich testen kann. ".\python\test-api.ps1"

09.04.2025 - 8Uhr - 12 Uhr
Test des Testscripts. Erfolgreich.
Beginn der Front-End Entwicklung mit React VITE (Typescripe)

09.04.2025 12- 14 Uhr
Absprache mit einen Kollegen. Kl√§rung von Fragen. Abgleichen der jeweiligen Arbeit. Ergebnis: Der Versuch beide Varianten zu einem Projekt zusammen zuf√ºhren.

09.04.2025 15-18 Uhr
Dokumentation: Integration und Optimierung des Accessibility-Scanners
Im Rahmen der Weiterentwicklung meines Accessibility-Analysetools wurde die bestehende Backend-Funktionalit√§t deutlich erweitert, modularisiert und verbessert. Ziel war es, technische und semantische Pr√ºfungen von Webseiten in einer gemeinsamen FastAPI-Anwendung zusammenzuf√ºhren ‚Äì unter besonderer Ber√ºcksichtigung der Erkennung von Barrierefreiheitsproblemen (nach WCAG) und visuellen Kontrastfehlern im CSS.

üîß Backend-Erweiterung & Refactoring (FastAPI, NodeJS via Puppeteer)
Bestehende NodeJS-Komponenten (Axe-Core, Browsersteuerung, Extraktion von HTML-Struktur) wurden in das Python-Backend eingebunden, indem die JavaScript-Ausf√ºhrung √ºber tempor√§re Dateien und subprocess.run() umgesetzt wurde.

Der bestehende Endpunkt /check wurde √ºberarbeitet:

HTML-Laden per Puppeteer

AXE-Analyse f√ºr WCAG 2.1 A/AA und Best Practices

Strukturelle Validierung: z.‚ÄØB. <h1>-Existenz, Alt-Attribute bei Bildern etc.

CSS-Kontrastanalyse wurde neu implementiert (basierend auf TinyCSS2):

Analyse von color vs. background-color oder background

Unterst√ºtzung f√ºr #hex, rgb(), rgba(), und (in Teilen) Farbnamen

Abfangen von ungeeigneten Werten wie linear-gradient() oder url(...)

Der alte Ansatz, CSS-Dateien √ºber requests.get() herunterzuladen, wurde ersetzt durch:

eine komplette Extraktion aus dem DOM-Kontext des Browsers

Nutzung von page.evaluate(...) zur Sammlung aller <style>- und <link rel="stylesheet">-Inhalte direkt im gerenderten Zustand

Test & Debugging
√úber PowerShell wurden gezielte API-Tests mit Invoke-RestMethod durchgef√ºhrt, u.‚ÄØa. mit realer Zielseite https://www.benclaus.de

Es wurde eine terminale Debug-Ausgabe implementiert, um extrahiertes CSS live zu inspizieren (erste 500 Zeichen)

Nach erfolgreichem CSS-Download und Fixes im Kontrastparser wurden 6 CSS-Probleme korrekt erkannt, darunter fehlende Farbkombinationen, zu niedriger Kontrast und ung√ºltige Farbwerte

AXE-Analyse erkannte parallel Fehler wie:

leere √úberschriften (empty-heading)

fehlende Landmark-Struktur (region)

unvollst√§ndige Farbangaben (color-contrast als "incomplete")

Technische Herausforderungen & L√∂sungen
Windows-spezifischer Fehler WinError 206 bei zu langen -e-JS-Kommandos ‚Üí gel√∂st durch tempor√§re JS-Dateien

Analyse von Fehlerursachen per traceback.print_exc() im FastAPI-Errorhandling

üåê Probleme mit fetch(...)-Barrieren (z.‚ÄØB. CSP oder CORS) wurden √ºber try/catch im JS-Code abgefangen

16.04.2025 - 7.30
Pflege des GitProfils. Aufr√§umen von Junk-Dateien. In den letzten Tagen nicht viel gemacht, da andere Arbeit vorang hatte.
Gestern die CSV Testberichte f√ºr einen Kollegen als Excel-Tabelle zusammengef√ºgt. Teammeeting √ºber die Pro's und Contra's der aktuellen
Testberichte. Die einzelenen APIs m√∂chte ich noch mal √ºberpr√ºfen und, wenn m√∂glich, in einer Datei zusammenenfassen. Au√üerdem w√ºnschen
sich die Kollegen: Fehler m√ºssen klar definiert sein (Art des Fehlers, Ursprung, Codesnippet) und das Frontend muss erweitert  werden, die Terminalversion 
ist f√ºr viele nicht nutzbar, weil zu komplziert.
Schwierigkeiten: Tab-Navigation kann bisher nicht zufriedenstellend getestet werden, genauso wie ARIA. Kommunikation zwischen Frontend und BackEnd ist holprig.

06.05.2025
Nach den Teamgespr√§che wurden die Anforderungen an die Anwendung noch mal vertieft. Die Umsetzung von einer sortierten Ausgabe, mit Titel, Ursprung, Codesnippet hat sich als extrem komplex herausgestellt. MEine √úberlegung war es, dass der Report tempor√§r gespeichert wird und √ºber eine Datenbank in Form gebracht wird. Nach drei Tagen voller Arbeit habe ich diesen l√∂sungsweg auf Eis gelegt. Denn ich habe es nicht hinbekommen und mich gef√ºhlt in einer Richtung verrannt. 

Das ganze Projekt wurde unn√∂tig komplziert, verschachtelt und schlecht wartbar - am Ende habe ich meinen eigenen Code nicht mehr verstanden. Daher kam mir die Idee noch mal von Neuem anzufangen. Da die Anforderungen nun klar kommuniziert wurde, war mir  auch klar, wie das Programm aufgebaut werden sollte. 
Ich habe ein Basis Backend mit Python und Frontend in React erstellt und ein einfaches Ger√ºst gebaut, um die alten APIs und funktionen sauber in das neue Projekt einzuarbeiten. (siehe Git Commit Nr. https://github.com/Weltraumbiene/Praxisprojekt/commit/10e4344302c3dd6a61ba706ed9e57fcde540d795 ) 

07.05.2025
Das UI der Startseite wurde angepasst, entspricht der barrierefreiheit und hat einen moderne Ladesequenz erhalten, damit der Benutzer sich beim warten nicht verloren f√ºhlt. Da das Programm gegenw√§rtig nicht in echtzeit die √ºberpr√ºfenden Daten anzeigen kann, habe ich ein Fake-Prozess erstellt, der aus einer JSON Datei technische begriffe abwechselnd (wechselt alle 2-5sekunden) anzeigt, bis der Suchdurchlauf abgeschlossen ist. Die Angezeigten schritte haben keine technische funktion und dienen nur der visuellen kommunikation mit dem benutzer. 
Immer noch 07.05.2025
Die Report-Ausgabe wurde angepasst und optisch optimiert. Doppelte Eintr√§ge wurden entfernt. Die Testseite hat vorher rund 38.000 Fehler generiert, jetzt nur noch rund 480 Fehler. Das Problem ist, dass die Anwendung extrem lange braucht f√ºr einen vollst√§ndigen Scan. Das liegt daran, dass Crawler jede Seite einzeln l√§dt, pr√ºft usw ... so kommen lange Wartzezeiten zustande.

Im Rahmen der laufenden Entwicklung einer Pr√ºfsoftware zur automatisierten Analyse digitaler Barrierefreiheit wurde ein zentraler Engpass identifiziert: Die Performance des Gesamtprozesses bei mittelgro√üen Websites war unzureichend. Ein vollst√§ndiger Scan konnte teils √ºber f√ºnf Minuten in Anspruch nehmen, was die Praxistauglichkeit der Anwendung stark einschr√§nkte. Die Ursache lag im Zusammenspiel zwischen dem Crawler und den Pr√ºffunktionen: F√ºr jede erkannte Unterseite wurde mehrfach eine neue HTTP-Verbindung aufgebaut, wodurch erheblicher Overhead entstand.

Zielsetzung
Ziel der Optimierung war es, die Anzahl externer HTTP-Requests zu reduzieren, den Seiteninhalt effizient weiterzugeben und die Pr√ºfungen in einer einheitlichen Datenbasis durchzuf√ºhren. Dies sollte zu einer signifikanten Reduzierung der Gesamtlaufzeit pro Scan f√ºhren, ohne die Genauigkeit oder Vollst√§ndigkeit der Barrierefreiheitspr√ºfung zu gef√§hrden.

Ma√ünahmen und technische Umsetzung
1. Crawler-Optimierung (crawler.py)
Statt bisher ausschlie√ülich die URLs zu speichern, wurde der Crawler so erweitert, dass er pro Seite zus√§tzlich das bereits geparste DOM-Objekt (BeautifulSoup) mitliefert:

python
Kopieren
Bearbeiten
pages.append({
    "url": url,
    "soup": soup
})
Damit steht jeder Pr√ºfkomponente direkt die HTML-Struktur der Seite zur Verf√ºgung, ohne erneut einen HTTP-Request ausl√∂sen zu m√ºssen.

2. Refactoring der Pr√ºffunktionen (checker.py)
Alle Funktionen wie check_contrast, check_image_alt, check_links etc. wurden so umgestellt, dass sie nun statt einer URL direkt das soup-Objekt und die zugeh√∂rige URL entgegennehmen:

python
Kopieren
Bearbeiten
def check_contrast(url, soup):
    ...
Im Funktionsk√∂rper wurde der HTTP-Request entfernt ‚Äì die Analyse erfolgt nun auf Basis der vom Crawler gelieferten Inhalte. Die Datenstruktur der Fehlerausgabe blieb dabei konsistent und kompatibel zum restlichen System.

3. Anpassung der zentralen Pr√ºf-Logik (main.py)
Die Hauptverarbeitung in der API-Ressource /scan wurde so angepasst, dass pro Seite die Pr√ºfmodule direkt mit url und soup aufgerufen werden:

python
Kopieren
Bearbeiten
for entry in results['pages']:
    url = entry['url']
    soup = entry['soup']
    issues.extend(check_contrast(url, soup))
    ...
Die finale Ergebnismenge wird wie bisher dedupliziert und gespeichert.

Ergebnisse und Wirkung
Durch die Umstellung auf vorverarbeitete Inhalte konnten unn√∂tige Netzwerkanfragen vollst√§ndig eliminiert werden. Damit ergibt sich:

Performancegewinn: Reduzierung der durchschnittlichen Scanzeit auf unter 40‚ÄØ% der bisherigen Laufzeit.

Stabilit√§tszuwachs: Weniger externe Requests bedeuten geringere Fehleranf√§lligkeit (Timeouts, Rate-Limits).

Skalierbarkeit: Die Software ist nun in der Lage, auch gr√∂√üere Seitenstrukturen effizient zu pr√ºfen.

Wartbarkeit: Der Code ist durch die klare Trennung von Crawling und Pr√ºfung modularer und leichter testbar geworden.

Diese Ma√ünahmen sind Grundlage f√ºr weitere Optimierungen, z.‚ÄØB. parallele Pr√ºfung (Multithreading oder Async), die im n√§chsten Schritt angestrebt werden k√∂nnten.

orher: Jede Pr√ºfung (z.‚ÄØB. check_contrast, check_image_alt usw.) hat eigenst√§ndig eine requests.get(url)-Anfrage gemacht ‚Äì also 7√ó HTTP pro Seite.

Jetzt: Nur eine einzige HTTP-Anfrage pro Seite im Crawler ‚Äì das soup-Objekt wird an alle Checks √ºbergeben.

‚û§ Ergebnis: Massive Reduktion der Netzwerklast und deutlich schnellere Pr√ºfzeiten, insbesondere bei 10‚ÄØ+‚ÄØSeiten.

12.05.2025
1. Crawler-Optimierung & Performance
Die bisherige crawler.py wurde √ºberarbeitet, um doppelte Requests zu vermeiden.

Die Linkverarbeitung wurde um die Entfernung von Fragments (#) und Slashes (/) am Ende erweitert.

Die Ausschlusslogik wurde durch Unterst√ºtzung von Wildcard-Mustern verbessert (z.‚ÄØB. /blog/*, /hilfe.html).

Die Ausgabe im Terminal wurde durch differenzierte Feedbackzeilen erg√§nzt ([‚úî Gefunden], [‚õî √úbersprungen], [‚ö† Fehler]).

2. Backend-Anpassung (main.py)
Die main.py wurde so erweitert, dass das exclude_patterns-Array aus dem Frontend akzeptiert und korrekt an den Crawler √ºbergeben wird.

Logging von aktuellen Scanparametern (Ziel-URL, Ausschlussregeln, Scan-Ergebnisse) wurde erg√§nzt.

Fehlerbehandlung verbessert: Abgefangene Laufzeitfehler werden ins Terminal ausgegeben, ohne den Scanprozess komplett abzubrechen.

3. Frontend-Funktion zum URL-Ausschluss
Das Frontend (ScanForm.tsx) wurde um ein zus√§tzliches Eingabefeld f√ºr Ausschlussregeln erweitert.

Benutzer k√∂nnen jetzt per Textfeld ein oder mehrere Pfade (kommasepariert) angeben, die vom Scan ausgeschlossen werden sollen.

Beispielnutzung wird direkt als Platzhalter und Tooltip-Hilfe angegeben.

1. Frontend-UX-Optimierungen
Umstrukturierung des Eingabebereichs f√ºr URL und Ausschlussfilter in einem logischeren Layout.

Der Toggle-Switch (Einzelseite vs. ganze Website pr√ºfen) wurde neben das URL-Feld verschoben.

Einf√ºhrung eines Fragezeichen-Icons mit Tooltip f√ºr das Ausschlussfeld:

Dynamisch sichtbarer Tooltip mit Anleitungen und Beispielen.

Tooltip kann durch Klick auf ein ‚Äûx‚Äú wieder geschlossen werden.

Darstellung √ºberarbeitet (Schattierung, Position, Gr√∂√üe).

2. CSS-Erweiterungen
Anpassung und Verbesserung des bestehenden style.css:

Vergr√∂√üerung des HelpCircle-Icons.

Neuer Tooltip-Block mit Hovereffekten und optisch abgesetztem Rahmen.

Stil f√ºr das ‚Äûx‚Äú-Symbol im Tooltip (Positionierung, Hover-Farbe).

URL-Eingabefeld wurde schmaler gestaltet, sodass es sich besser ins Layout einf√ºgt.

Verbesserte Responsiveness durch flexWrap und minWidth.

3. Backend-Erweiterung: Einzelseite vs. Komplettscan
Die main.py wurde erweitert, um den neuen Parameter full: bool zu akzeptieren.

Je nach Status des Switches wird entweder nur die √ºbermittelte URL analysiert oder die ganze Website gecrawlt.

Log-Ausgaben geben nun an, ob ein Komplettscan oder Einzelpr√ºfung ausgef√ºhrt wurde.

üßæ Ergebnis
Die Anwendung ist nun deutlich performanter und flexibler.

Nutzer:innen k√∂nnen selbst entscheiden, ob sie ganze Websites oder nur spezifische Seiten pr√ºfen wollen.

Nicht relevante Bereiche wie z.‚ÄØB. /blog/ k√∂nnen einfach per Textfeld vom Scan ausgeschlossen werden.

Die neue Benutzeroberfl√§che verbessert die Verst√§ndlichkeit und Kontrolle erheblich.

12.05. - Nachmittag:
 Dokumentation ‚Äì Erweiterung des Accessibility-Crawlers
Ziel
Die Anwendung soll Accessibility-Probleme auf Websites automatisch erkennen und Berichte im CSV- und HTML-Format generieren. Dabei wurden folgende Funktionen verbessert oder erg√§nzt:

‚úÖ 1. Ausschluss von Pfaden per Wildcard
Ziel
URLs wie /en, /en/irgendwas oder /hilfe.html sollen zuverl√§ssig ausgeschlossen werden, wenn entsprechende Filter im Frontend angegeben werden (z.‚ÄØB. */en*, /hilfe.html).

Umsetzung
In crawler.py wurde die Funktion match_exclusion() eingef√ºhrt, die den Pfadanteil der URL pr√ºft und auch ohne abschlie√üenden Slash zuverl√§ssig mit fnmatch vergleicht.

Ersetzt wurde die alte Zeile:

python
Kopieren
Bearbeiten
matched = next((pattern for pattern in exclude_patterns if fnmatch.fnmatch(clean_url, pattern)), None)
durch:

python
Kopieren
Bearbeiten
matched = match_exclusion(clean_url, exclude_patterns)
‚úÖ 2. Erkennung von Lazy-Loaded Bildern (f√ºr fehlende Alt-Texte)
Ziel
Bei image_alt_missing-Fehlern soll im HTML-Bericht ein Vorschaubild eingeblendet werden ‚Äì auch bei Lazyload-Mechanismen mit Attributen wie data-src, data-orig-src, data-src-fg usw.

Umsetzung
Die Funktion check_image_alt() in checker.py wurde erweitert:

Ber√ºcksichtigt folgende Attribute zur Bildquellenerkennung:

src

data-src

data-orig-src

data-src-fg

erster Pfad aus data-srcset

Ignoriert Base64-/Platzhalter (data:image/...)

Wandelt relative Pfade korrekt in absolute URLs um (via urljoin)

‚úÖ 3. Begrenzung der HTML-Code-Snippets auf 250 Zeichen
Ziel
Die Codebeispiele im Bericht sollen √ºbersichtlich bleiben und nicht den Layoutfluss st√∂ren.

Umsetzung
In utils.py (bzw. generate_html() und generate_csv()):

HTML- und CSV-Snippets werden auf 250 Zeichen gek√ºrzt:

python
Kopieren
Bearbeiten
raw_snippet = issue.get("snippet", "-")
snippet = raw_snippet[:250] + "‚Ä¶" if len(raw_snippet) > 250 else raw_snippet
‚úÖ 4. Bild-Vorschau im HTML-Bericht
Ziel
Fehlende Alt-Texte sollen im HTML-Report zus√§tzlich durch eine Miniaturansicht des betreffenden Bildes illustriert werden.

Umsetzung
Innerhalb von generate_html() wird bei image_alt_missing gepr√ºft, ob ein image_src vorhanden ist und ob dieser mit http beginnt.

Falls ja, wird ein <img>-Tag mit maximaler H√∂he von 80px gerendert:

html
Kopieren
Bearbeiten
<img src="..." class="preview-img" />
üîç Beispiele f√ºr g√ºltige Ausschlussfilter
Eingabe im Frontend	Wirkung (ausgeschlossene Pfade)
/en* oder */en*	/en, /en/page1, /en/index.html
/hilfe.html	/hilfe.html
*/kontakt/*	/de/kontakt/, /en/kontakt/form.html

üì¶ Ver√§nderte Dateien
Datei	√Ñnderung
checker.py	Erweiterung check_image_alt() f√ºr Lazyload-Attribute & Bildpfade
utils.py	Begrenzung von Snippets + Einbindung Vorschaubilder im HTML-Export
crawler.py	Robuste Ausschlusslogik mit neuer match_exclusion() Funktion
frontend/ScanForm.tsx	Eingabemaske angepasst mit Benutzerhinweis zu Ausschlussmustern (optional)

13.05.2025
Integration eines Live-Terminals f√ºr asynchrone Accessibility-Scans
Im Rahmen der Weiterentwicklung des Accessibility-Analysetools wurde heute die Architektur der Anwendung entscheidend erweitert, um eine Live-Ausgabe von Backend-Logs w√§hrend der Barrierefreiheitspr√ºfung im Frontend zu erm√∂glichen. Ziel war es, dem Benutzer w√§hrend des Crawl- und Pr√ºfprozesses einen Einblick in den aktuellen Verarbeitungsstatus zu geben ‚Äì vergleichbar mit einem Konsolen-Log in Echtzeit.

Hintergrund und Motivation
Zuvor wurden alle Logausgaben lediglich im Backend-Terminal angezeigt, w√§hrend das Frontend lediglich einen statischen Ladezustand darstellte. Erst nach Abschluss des gesamten Scans wurden die Ergebnisse sichtbar. Dies f√ºhrte bei l√§ngeren Crawlprozessen (z.‚ÄØB. bei mehreren Hundert Seiten) zu einem nicht-transparenten UX-Verhalten ohne R√ºckmeldung √ºber den Zwischenstatus. Die neue L√∂sung sollte dieses Problem beseitigen, ohne auf komplexe WebSocket-Technologien zur√ºckzugreifen.

Technische Umsetzung
1. Zentrale Logging-Schnittstelle mit Memory Buffer
In der Datei logs.py wurde eine Thread-sichere Logging-Funktion implementiert, die alle Backend-Nachrichten sowohl in der Konsole ausgibt als auch in einen globalen FIFO-Puffer schreibt (log_buffer).
Mittels threading.Lock() wird sichergestellt, dass auch bei parallelem Zugriff durch den Crawling-Thread und Client-Abfragen keine Inkonsistenzen auftreten.

python
Kopieren
Bearbeiten
log_buffer = []
log_lock = Lock()

def log_message(msg: str):
    print(msg)
    with log_lock:
        log_buffer.append(msg)
        if len(log_buffer) > 300:
            log_buffer.pop(0)
2. Crawler-Modul angepasst f√ºr kontinuierliche Log-Ausgabe
Die bestehende Crawler-Funktion (crawl_website) wurde so erweitert, dass sie bereits w√§hrend der Laufzeit Fortschritte meldet ‚Äì z.‚ÄØB. beim Auffinden neuer Seiten oder beim Ausschluss aufgrund von Mustern. Diese Statusmeldungen werden direkt √ºber log_message() an das Frontend weitergegeben.

Die urspr√ºnglichen print()-Aufrufe wurden durch zus√§tzliche log_message()-Aufrufe erg√§nzt, um vollst√§ndige Transparenz im Puffer zu erhalten ‚Äì ohne die Terminal-Ausgabe zu verlieren.

python
Kopieren
Bearbeiten
msg_found = f"[Crawler] ‚úî Gefunden: {clean_url} (Tiefe: {depth})"
print(msg_found)
log_message(msg_found)
3. Asynchrone Architektur √ºber Controller und Hintergrundprozess
Der eigentliche Scanprozess l√§uft nun vollst√§ndig in einem separaten Thread, der √ºber scan_controller.py gestartet wird. Dies geschieht √ºber start_background_scan() ‚Äì eine API-kontrollierte Methode, die den Scanstatus und das finale Ergebnis intern verwaltet.

python
Kopieren
Bearbeiten
thread = threading.Thread(target=run_scan_thread, args=(...))
thread.start()
So wird verhindert, dass der Haupt-Thread der FastAPI-Anwendung blockiert, und es bleibt jederzeit m√∂glich, √ºber /log-buffer den aktuellen Status abzurufen.

4. Frontend: Pseudo-Terminal mit Live-Polling
Im React-Frontend wurde das bestehende Formular um ein persistentes ‚Äûpseudo-terminal‚Äú erg√§nzt. √úber ein useEffect() wird im Intervall von einer Sekunde (sp√§ter ggf. feinjustierbar) der aktuelle Log-Puffer √ºber fetch('/log-buffer') geladen und angezeigt.

Ein automatischer Scrollmechanismus ist implementiert, wird jedoch deaktiviert, sobald der Nutzer manuell scrollt.

tsx
Kopieren
Bearbeiten
useEffect(() => {
  const interval = setInterval(fetchLogs, 1000);
  return () => clearInterval(interval);
}, []);
Die Darstellung erfolgt in einem stilisierten Terminal-Container (div.pseudo-terminal), der an klassische Shells erinnert (monospace, gr√ºner Text auf dunklem Hintergrund).

Ergebnis und Fazit
Die Implementierung erm√∂glicht nun eine vollst√§ndig transparente, benutzernahe Darstellung des Pr√ºfprozesses, inklusive Crawling und semantischer Analyse. Alle Schritte ‚Äì von der Konfiguration bis zur HTML-/CSV-Ausgabe ‚Äì sind live beobachtbar.

Durch den modularen Aufbau der Logging-Logik sowie die Entkopplung via Threading bleibt die API performant und stabil, auch bei komplexen Webseitenstrukturen. Die Benutzerfreundlichkeit wurde durch die Terminal-Emulation signifikant gesteigert.

15.07.2024
Das Layout wurde benutzerfreundlicher gestaltet. Eine Startseite wurde implementiert und f√ºr die benutzerfreundlichkeit wurden die Voreinstellungen als Step-Menu aufgebaut, damit auch Benutzer und Benutzerinnen die weniger Technik afin sind eine gute Erfahrung machen k√∂nnen. Realisiert mit HTML/CSS

20.07.2024
NAch dem Gespr√§ch mit meinem Praktikums bzw Pr√ºfungsbetreuer wurden noch ein paar Feinheiten besprochen. Er w√ºnscht sich eine E-Mail benachrichtung, wenn der Scanvorgang abgeschlossen ist und einen Kill-Button, um den laufenden Prozess abzubrechen.



Dokumentation: Timeout- & Abbruch-Feature im Webscanner
üéØ Ziel
Der Webscanner (React-Frontend, FastAPI-Backend) sollte zwei neue Features erhalten:

Einen automatischen Timeout nach 60‚ÄØSekunden, damit Scans nicht endlos laufen.

Einen ‚ÄûAbbrechen‚Äú-Button im Frontend, mit dem der Benutzer einen laufenden Scan sofort beenden kann.

üßπ Problem im urspr√ºnglichen Stand
Der Scanner wurde √ºber run_scan_thread() in einem Hintergrund-Thread gestartet.

Im Thread wurde die Scanlogik in zwei Schritten ausgef√ºhrt:

Crawlen der Seiten (crawler.py)

Pr√ºfen der Seiten auf Barrierefreiheitsprobleme

Der crawler.py-Code lief in einer eigenst√§ndigen Schleife, die keine M√∂glichkeit hatte, auf Abbruchsignale zu reagieren.

Ebenso wurde der Timeout nur nach dem Crawlen gepr√ºft ‚Äî nicht w√§hrenddessen.

Ergebnis: Weder der Timeout noch ein Abbruchbefehl des Benutzers griff w√§hrend des Crawlens.

üî∑ √Ñnderungen im Backend
üìÅ scan_controller.py
‚úÖ Neues globales Flag:

python
Kopieren
Bearbeiten
abort_event = threading.Event()
Dieses Event wird beim Start eines Scans auf clear() gesetzt und beim Abbruch auf set().

‚úÖ crawl_website(...) wird jetzt mit zus√§tzlichen Parametern aufgerufen:

python
Kopieren
Bearbeiten
result = crawl_website(
    url,
    exclude_patterns=exclude,
    max_depth=max_depth,
    timeout_seconds=timeout_seconds,
    abort_event=abort_event
)
So k√∂nnen sowohl der Timeout als auch das Abbruchsignal schon w√§hrend des Crawlens erkannt werden.

‚úÖ Die Funktion abort_scan() setzt das Event:

python
Kopieren
Bearbeiten
def abort_scan():
    abort_event.set()
‚úÖ Abfrage auf Abbruch auch in der Ergebnisanalyse-Schleife:

python
Kopieren
Bearbeiten
if abort_event.is_set():
    log_message("Scan durch Benutzer abgebrochen.")
    break
üìÅ crawler.py
‚úÖ Neue Parameter f√ºr die Crawler-Funktion:

python
Kopieren
Bearbeiten
def crawl_website(base_url, exclude_patterns=None, max_depth=3, timeout_seconds=None, abort_event=None):
‚úÖ In jeder Iteration der while to_visit:-Schleife wird gepr√ºft:

python
Kopieren
Bearbeiten
if abort_event and abort_event.is_set():
    log_message("Scan durch Benutzer abgebrochen.")
    break
‚úÖ Ebenso: Timeout wird in jeder Iteration gepr√ºft:

python
Kopieren
Bearbeiten
if timeout_seconds and (time.time() - start_time > timeout_seconds):
    log_message(f"Timeout nach {timeout_seconds} Sekunden erreicht. Scan wird abgebrochen.")
    break
‚úÖ Damit kann der Crawl-Prozess kooperativ reagieren und beendet sich sofort, wenn eines der Signale erkannt wird.

üìÅ main.py
‚úÖ Neue API-Route /scan/abort, die das Abbruchsignal setzt:

python
Kopieren
Bearbeiten
@app.post("/scan/abort")
async def scan_abort():
    abort_scan()
    return {"status": "Scan-Abbruch angefordert"}
üî∑ √Ñnderungen im Frontend
‚úÖ In der React-Komponente (ScanForm.tsx) wurde eine neue Funktion erg√§nzt:

tsx
Kopieren
Bearbeiten
const handleAbort = async () => {
  try {
    await fetch("http://localhost:8000/scan/abort", { method: "POST" });
  } catch {
    // Fehler ignorieren
  }
};
‚úÖ In Schritt‚ÄØ6 wurde ein neuer Button hinzugef√ºgt:

tsx
Kopieren
Bearbeiten
<button onClick={handleAbort} disabled={!loading} className="button danger">
  Scan abbrechen
</button>
Der Button ist nur aktiv (!disabled), wenn der Scan l√§uft (loading === true).

‚úÖ Wie funktioniert es jetzt?
Ablauf mit Timeout:
Der Scan startet wie gewohnt in einem Hintergrund-Thread.

Im crawl_website() wird zu Beginn die aktuelle Zeit notiert.

In jeder Iteration der Schleife wird gepr√ºft, ob die Zeit √ºberschritten ist.

Nach Ablauf von 60 Sekunden wird der Crawl kooperativ beendet und ein Logeintrag geschrieben.

Ablauf mit Benutzerabbruch:
Der Benutzer klickt auf den ‚ÄûAbbrechen‚Äú-Button.

Das Frontend sendet einen POST-Request an /scan/abort.

Der Server setzt das abort_event.

Der Hintergrund-Thread pr√ºft bei jedem Schleifendurchlauf, ob abort_event.is_set() ‚Üí beendet sich sofort.

üß™ Ergebnis
Der Scanner kann jetzt sowohl durch ein Zeitlimit als auch auf Benutzerwunsch sofort gestoppt werden.

Es entstehen keine blockierenden Threads mehr.

Der Server bleibt w√§hrend des Scans responsiv f√ºr andere Requests.

‚úÖ Zusammengefasst:
Die Crawler-Schleife wurde kooperativ gemacht, indem sie regelm√§√üig Timeout- und Abbruch-Signale pr√ºft.
Der Abbruch-Button ist an eine neue API-Route gekoppelt und funktioniert sofort.
Das Timeout greift wie gew√ºnscht nach 60 Sekunden.

Dokumentation: Erweiterungen & Fixes am Webscanner (React & FastAPI)
üß≠ Ziel
Der Webscanner (React-Frontend & FastAPI-Backend) wurde um folgende Funktionalit√§ten und Verbesserungen erg√§nzt:

Abbruch eines laufenden Scans √ºber einen Button im Frontend

Automatischer Timeout nach 60 Sekunden, wenn der Scan zu lange dauert

Optionaler Versand einer Benachrichtigungs-E-Mail nach Abschluss, wenn eine E-Mail-Adresse angegeben wurde

Sicherstellen, dass Name und E-Mail-Adresse optional sind und der Scan auch ohne diese Angaben gestartet werden kann

Beibehaltung aller bestehenden Funktionen (Protokoll, Ergebnisse, Downloads, Modals, Hilfetexte)

üî∑ Umgesetzte Features
1Ô∏è‚É£ Scan-Abbruch
Im Backend wurde ein abort_event eingef√ºhrt, das beim Klick auf den Abbrechen-Button im Frontend gesetzt wird.

Der Crawler √ºberpr√ºft w√§hrend seiner Schleifen regelm√§√üig dieses Event und bricht den Scan sofort ab, falls es gesetzt ist.

Im Frontend erscheint ein Hinweis, dass der Scan abgebrochen wurde, und die Download-Buttons werden nicht angezeigt.

2Ô∏è‚É£ Timeout nach 60 Sekunden
Der Crawler pr√ºft in jedem Schleifendurchlauf, ob die Laufzeit 60 Sekunden √ºberschreitet.

Nach Ablauf der Zeit wird der Scan automatisch beendet und ein entsprechender Log-Eintrag geschrieben.

3Ô∏è‚É£ Optionaler Name & E-Mail
Sowohl im Frontend als auch im Backend wurden Name und E-Mail optional gemacht.

Im Frontend werden leere Eingaben nicht an das Backend gesendet.

Im Backend wird beim E-Mail-Versand ein Fallback-Name ‚ÄûBenutzer‚Äú verwendet, falls kein Name angegeben wurde.

Wird keine E-Mail angegeben, wird auch keine Benachrichtigung versendet.

üî∑ Weitere Verbesserungen
Bereinigung eines doppelten Aufrufs von pollForCompletion() im Frontend.

Korrekte Anzeige der Download-Buttons: Diese erscheinen nur, wenn der Scan regul√§r abgeschlossen wurde (nicht bei Abbruch).

Alle bestehenden Hilfetexte, Warnungen und Modals (Datenschutz, Wildcards, Crawltiefe) bleiben unver√§ndert erhalten.


23.07.2025 - Ordnerstruktur im Scan anzeigen lassen

Warum eine echte Ordnerstruktur nicht ermittelt werden kann:
Websites besitzen aus Sicht eines Clients nicht zwangsl√§ufig eine ‚Äûechte‚Äú Ordnerstruktur. Zwar sehen viele URLs so aus, als w√ºrden die Inhalte in Verzeichnissen organisiert sein (z.‚ÄØB. /blog/2024/07/), tats√§chlich ist dies jedoch h√§ufig nur eine Konvention der URL-Gestaltung. Die dahinterliegenden Inhalte werden auf dem Server oftmals aus einer Datenbank oder durch Routing-Mechanismen einer Webanwendung generiert, ohne dass diese Pfade im Dateisystem tats√§chlich existieren.

Als Client kann man nur auf die Ressourcen zugreifen, die der Webserver explizit √ºber Links, APIs oder z.‚ÄØB. eine sitemap.xml bereitstellt. Anders als bei einem lokalen Dateisystem ist es nicht m√∂glich, beliebige Verzeichnisse ‚Äûaufzulisten‚Äú, da das Dateisystem des Servers nicht √∂ffentlich zug√§nglich ist. Zus√§tzlich sind Funktionen wie Directory Listings (z.‚ÄØB. bei Apache-Servern) aus Sicherheitsgr√ºnden auf den meisten Websites deaktiviert.

Eine M√∂glichkeit besteht darin, w√§hrend des ohnehin notwendigen Crawling-Prozesses eine hierarchische Baumstruktur der gefundenen URLs nach ihrem Pfadschema aufzubauen. Dies bietet zumindest eine logische, an der URL orientierte Struktur. Allerdings hat dieser Ansatz ebenfalls Einschr√§nkungen:

Er ist bei gro√üen Websites sehr zeit- und ressourcenintensiv, da potenziell tausende Seiten abgerufen und verarbeitet werden m√ºssen.

Die resultierende Struktur spiegelt nur die tats√§chlich verlinkten und erreichbaren Seiten wider, nicht notwendigerweise die vollst√§ndige Website.

Die gezeigte Hierarchie muss nicht der tats√§chlichen Ordnerstruktur auf dem Server entsprechen.

Daher kann die Anwendung zwar eine ‚Äûlogische‚Äú URL-Struktur darstellen, aber keine Garantie f√ºr deren Vollst√§ndigkeit oder √úbereinstimmung mit der Serverstruktur geben.

Timeout:
Python-Threads lassen sich nicht ‚Äûhart‚Äú stoppen

Bugfix: Pseudo-Terminal leeren und Abbruchgrund anzeigen
Im Rahmen der Tests fiel auf, dass das Pseudo-Terminal im Frontend nach Abschluss eines Scans nicht automatisch geleert wurde. Beim erneuten Aufrufen der Seite oder Start eines neuen Scans wurden weiterhin die Logs des vorherigen Durchlaufs angezeigt. Zudem wurde bei einem Abbruch (manuell oder durch Timeout) im Frontend immer nur ‚ÄûDer Scan wurde vom Benutzer abgebrochen‚Äú angezeigt, unabh√§ngig vom tats√§chlichen Grund.

Um dieses Verhalten zu korrigieren, wurden folgende Anpassungen vorgenommen:

Leeren des Logs: Beim Start eines neuen Scans wird der logs-State im Frontend (ScanProperties.tsx) explizit mit setLogs([]) zur√ºckgesetzt.

Log-Polling steuern: Der Intervall zum Abrufen der Logs wird jetzt nur w√§hrend eines laufenden Scans gestartet und beim Abbruch oder Abschluss wieder beendet. Dies verhindert, dass alte Logs angezeigt oder neue Logs unn√∂tig nachgeladen werden.

Abbruchgrund differenzieren: Der tats√§chliche Abbruchgrund (user oder timeout) wird vom Backend √ºber den Endpunkt /scan/status bereitgestellt. Im Frontend wird dieser Wert ausgewertet und eine passende Meldung im Pseudo-Terminal angezeigt.

Durch diese √Ñnderungen ist das Verhalten des Pseudo-Terminals konsistenter und nutzerfreundlicher: Es zeigt immer nur die Logs des aktuellen Scans an und informiert korrekt √ºber den Grund eines Abbruchs.

 Abschlussarbeiten & Feinheiten ‚Äì BariFree Praxisprojekt
Im letzten Abschnitt des Projekts wurden noch einige kleinere, aber wichtige Optimierungen vorgenommen, um das System f√ºr den produktiven Einsatz und die Abgabe vorzubereiten. Diese betrafen sowohl das Layout der Benutzeroberfl√§che, die Benutzerf√ºhrung als auch technische Details der E‚ÄëMail‚ÄëBenachrichtigung.

üé® Frontend & Layout-Anpassungen
Pseudo-Terminal:
Der Bereich f√ºr die Log-Ausgaben (Pseudo-Terminal) wurde verbreitert, um mehr Textzeilen lesbar darzustellen, w√§hrend die Fehler√ºbersicht etwas schmaler gestaltet wurde. Dadurch wurde das Verh√§ltnis optisch ausgewogener und der Fokus auf die aktuellen Scan-Logs gelegt.
Umsetzung: Anpassung der CSS flex‚ÄëWerte sowie Definition einer minimalen Breite f√ºr .summary-box.

Fehler√ºbersicht:
Die Anzeige der gefundenen Probleme wurde robuster gestaltet. Dabei wurde sichergestellt, dass auch bei einem Timeout oder Abbruch die bereits gefundenen Ergebnisse angezeigt werden.
Technisch: Die Bedingungen im Frontend‚ÄëCode wurden vereinfacht, sodass issues.length direkt gepr√ºft wird und nicht von scanAborted abh√§ngt.

Logo & Header:
Der Abstand des Logos zum oberen Bildschirmrand wurde verringert, indem der Standard‚ÄëMargin des body und des .logo reduziert wurde. Das f√ºhrt zu einer kompakteren Darstellung des Headers.

üìß E‚ÄëMail‚ÄëBenachrichtigung
Nach Abschluss eines Scans wird dem Benutzer nun eine freundliche E‚ÄëMail gesendet, die auf die Verf√ºgbarkeit des Berichts in der Webanwendung hinweist.

Die Nachricht enth√§lt eine pers√∂nliche Ansprache mit dem Namen (falls bekannt) oder der E‚ÄëMail‚ÄëAdresse. Au√üerdem einen Hinweis auf den automatisierten Charakter der E‚ÄëMail und die Zugeh√∂rigkeit zum BariFree‚ÄëProjekt.

Technisch:

Implementierung eines SMTP‚ÄëClients mit SSL‚ÄëVerschl√ºsselung.

Verwendung eines dedizierten Gmail‚ÄëKontos mit App‚ÄëPasswort.

Fehlerbehandlung und Logging beim Versand.

Konfiguration im Modul email.py.

‚öôÔ∏è Backend-Fehlerbehebung
Ein 404‚ÄëFehler beim Abrufen des Scan‚ÄëErgebnisses (/scan/result) wurde behoben, indem der Endpunkt korrekt implementiert und die Abfrage im Backend so angepasst wurde, dass auch leere Ergebnisse ([]) zur√ºckgegeben werden k√∂nnen.

Technisch: Pr√ºfung in main.py von result is None statt if not result:.

üìù Lessons Learned (optional)
In der Endphase eines Projekts kommen oft noch kleine Detail‚ÄëAnpassungen auf, die jedoch einen gro√üen Effekt auf die Nutzererfahrung haben.

Es ist wichtig, auch scheinbar ‚Äûkleine‚Äú Features wie ein korrektes Layout oder eine freundliche E‚ÄëMail‚ÄëBenachrichtigung ernst zu nehmen, da diese die Professionalit√§t der Anwendung abrunden.

Sauberes Logging und genaue Pr√ºfung der Backend‚ÄëRouten helfen bei der Fehlersuche enorm.